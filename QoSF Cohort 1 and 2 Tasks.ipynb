{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Duplicate key in file '/Users/minhpham/.matplotlib/matplotlibrc' line #2.\n",
      "Duplicate key in file '/Users/minhpham/.matplotlib/matplotlibrc' line #3.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sympy import Matrix, init_printing\n",
    "\n",
    "import qiskit\n",
    "from qiskit import *\n",
    "\n",
    "# Representing Data\n",
    "from qiskit.providers.aer import QasmSimulator, StatevectorSimulator, UnitarySimulator\n",
    "from qiskit.tools.visualization import plot_histogram, plot_state_city, plot_bloch_multivector\n",
    "\n",
    "# Monitor Job on Real Machine\n",
    "from qiskit.tools.monitor import job_monitor\n",
    "\n",
    "from functools import reduce # perform sucessive tensor product\n",
    "\n",
    "# Calculating cost\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generating random unitary matrix\n",
    "from scipy.stats import unitary_group\n",
    "\n",
    "# Measure run time\n",
    "import time\n",
    "\n",
    "# Almost Equal\n",
    "from numpy.testing import assert_almost_equal as aae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "Implement a circuit which returns $|00\\rangle$ and $|11 \\rangle$ with equal probability.\n",
    "\n",
    "Requirements :\n",
    "\n",
    "- Circuit should consist only of CNOTs, RXs and RYs. \n",
    "\n",
    "- Start from all parameters in parametric gates being equal to 0 or randomly chosen. \n",
    "\n",
    "- You should find the right set of parameters using gradient descent (you might use more advanced optimization methods if you like). \n",
    "\n",
    "- Simulations must be done with sampling - i.e. limited number of measurements per iteration and noise. \n",
    "\n",
    "- Compare the results for different numbers of measurements: 1, 10, 100, 1000. \n",
    "\n",
    "Bonus question:\n",
    "\n",
    "- How to make sure you produce state $|00\\rangle +|11 \\rangle$ and not $|00\\rangle - |11\\rangle$ ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Create a quantum circuit with Hamadamard and CX\n",
    "\n",
    "    2. Decompose Hadamard into R_X and R_Y using gate decomposition\n",
    "\n",
    "    3. Draw Neural Network graph\n",
    "\n",
    "    4. Create data to train/test\n",
    "\n",
    "    5. Initialized weights\n",
    "\n",
    "    6. Implement Forward Propagation\n",
    "\n",
    "    7. Define Cost function\n",
    "\n",
    "    8. Implement Backpropagation\n",
    "\n",
    "    9. Implement Gradient Descent to updates the weights in each layers\n",
    "    \n",
    "    10. Check that each layer is implemented correctly\n",
    "    \n",
    "    11. Train Model\n",
    "    \n",
    "    12. Evaluate Model\n",
    "    \n",
    "    13. Full Optimization Implementation\n",
    "    \n",
    "    14. Sampling on Quantum Circuit\n",
    "    \n",
    "\n",
    "Optional:\n",
    "\n",
    "    1. Implement Adam Optimization\n",
    "    2. Implement General Neural Network\n",
    "    3. Implement Forward Prop with simulation\n",
    "    4. Bonus question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful Linear Algebra Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices\n",
    "I = np.array([[1, 0], [0, 1]])\n",
    "X = np.array([[0, 1], [1, 0]])\n",
    "Y = np.array([[0, -1j], [1j, 0]])\n",
    "Z = np.array([[1, 0], [0, -1]])\n",
    "H = 1/np.sqrt(2)*np.array([[1, 1], [1, -1]])\n",
    "\n",
    "CX = np.array([[1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 1, 0, 0]])\n",
    "\n",
    "# Eigenvectors of Pauli Matrices\n",
    "zero = np.array([[1], [0]]) # Z plus basis state\n",
    "one = np.array([[0], [1]]) # Z plus basis state\n",
    "\n",
    "plus = np.array([[1], [1]])/np.sqrt(2) # X plus basis state\n",
    "minus = np.array([[1], [-1]])/np.sqrt(2) # X minus basis state\n",
    "\n",
    "up = np.array([[1], [1j]])/np.sqrt(2) # Y plus basis state\n",
    "down = np.array([[1], [-1j]])/np.sqrt(2) # Y plus basis state\n",
    "\n",
    "# Bell States\n",
    "B00 = np.array([[1], [0], [0], [1]])/np.sqrt(2) # Bell of 00\n",
    "B01 = np.array([[1], [0], [0], [-1]])/np.sqrt(2) # Bell of 01\n",
    "B10 = np.array([[0], [1], [1], [0]])/np.sqrt(2) # Bell of 10\n",
    "B11 = np.array([[0], [-1], [1], [0]])/np.sqrt(2) # Bell of 11\n",
    "\n",
    "# Rn Matrix Function\n",
    "Rx = lambda theta: np.array([[np.cos(theta/2), -1j*np.sin(theta/2)], [-1j*np.sin(theta/2), np.cos(theta/2)]])\n",
    "Ry = lambda theta: np.array([[np.cos(theta/2), -np.sin(theta/2)], [np.sin(theta/2), np.cos(theta/2)]])\n",
    "Rz = lambda theta: np.array([[np.exp(-1j*theta/2), 0], [0, np.exp(1j*theta/2)]])\n",
    "\n",
    "# Tensor Product of 2+ matrices/ vectors\n",
    "tensor = lambda *initial_state: reduce(lambda x, y: np.kron(x, y), initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### View Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view(mat):\n",
    "    display(Matrix(mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Unitary/StateVector Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function return the statevector or the unitary of an inputted circuit\n",
    "\n",
    "def get(circ, types = 'unitary', nice = True):\n",
    "    \n",
    "    if types == 'statevector':\n",
    "        backend = BasicAer.get_backend('statevector_simulator')\n",
    "        out = execute(circ, backend).result().get_statevector()\n",
    "    else: \n",
    "        backend = BasicAer.get_backend('unitary_simulator')\n",
    "        out = execute(circ, backend).result().get_unitary()\n",
    "        \n",
    "    if nice:\n",
    "        display(Matrix(np.round(out, 10))) \n",
    "    else:\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_neural_net(ax, left, right, bottom, top, layer_sizes):\n",
    "    '''\n",
    "    Draw a neural network cartoon using matplotilb.\n",
    "    \n",
    "    :usage:\n",
    "        >>> fig = plt.figure(figsize=(12, 12))\n",
    "        >>> draw_neural_net(fig.gca(), .1, .9, .1, .9, [4, 7, 2])\n",
    "    \n",
    "    :parameters:\n",
    "        - ax : matplotlib.axes.AxesSubplot\n",
    "            The axes on which to plot the cartoon (get e.g. by plt.gca())\n",
    "        - left : float\n",
    "            The center of the leftmost node(s) will be placed here\n",
    "        - right : float\n",
    "            The center of the rightmost node(s) will be placed here\n",
    "        - bottom : float\n",
    "            The center of the bottommost node(s) will be placed here\n",
    "        - top : float\n",
    "            The center of the topmost node(s) will be placed here\n",
    "        - layer_sizes : list of int\n",
    "            List of layer sizes, including input and output dimensionality\n",
    "    '''\n",
    "    n_layers = len(layer_sizes)\n",
    "    v_spacing = (top - bottom)/float(max(layer_sizes))\n",
    "    h_spacing = (right - left)/float(len(layer_sizes) - 1)\n",
    "    # Nodes\n",
    "    for n, layer_size in enumerate(layer_sizes):\n",
    "        layer_top = v_spacing*(layer_size - 1)/2. + (top + bottom)/2.\n",
    "        for m in range(layer_size):\n",
    "            circle = plt.Circle((n*h_spacing + left, layer_top - m*v_spacing), v_spacing/4.,\n",
    "                                color='w', ec='k', zorder=4)\n",
    "            ax.add_artist(circle)\n",
    "    # Edges\n",
    "    for n, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "        layer_top_a = v_spacing*(layer_size_a - 1)/2. + (top + bottom)/2.\n",
    "        layer_top_b = v_spacing*(layer_size_b - 1)/2. + (top + bottom)/2.\n",
    "        for m in range(layer_size_a):\n",
    "            for o in range(layer_size_b):\n",
    "                line = plt.Line2D([n*h_spacing + left, (n + 1)*h_spacing + left],\n",
    "                                  [layer_top_a - m*v_spacing, layer_top_b - o*v_spacing], c='k')\n",
    "                ax.add_artist(line)\n",
    "                \n",
    "                \n",
    "# Draw neural network for this problem\n",
    "def draw_nn():\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "    fig.text(0.15, 0.76, 'Input Layer', fontsize = 12)\n",
    "    fig.text(0.75, 0.76, 'Output Layer', fontsize = 12)\n",
    "\n",
    "    # Input Layer\n",
    "    fig.text(0.177, 0.65, 'x_0', fontsize = 16)\n",
    "    fig.text(0.177, 0.35, 'x_1', fontsize = 16)\n",
    "    \n",
    "    # Output Layer\n",
    "    fig.text(0.795, 0.65, 'y_0', fontsize = 16)\n",
    "    fig.text(0.795, 0.35, 'y_1', fontsize = 16)\n",
    "\n",
    "    ax = fig.gca()\n",
    "    ax.axis('off')\n",
    "    draw_neural_net(ax, .1, .9, .1, .9, [2, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Build Bell Circuit\n",
    "\n",
    "Create a quantum circuit with Hadamard and CX\n",
    "\n",
    "Typically, a circuit to create the $\\frac{1}{\\sqrt{2}}(|00\\rangle + |11\\rangle)$ looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.7071067812 & 0.7071067812 & 0 & 0\\\\0 & 0 & 0.7071067812 & -0.7071067812\\\\0 & 0 & 0.7071067812 & 0.7071067812\\\\0.7071067812 & -0.7071067812 & 0 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0.7071067812,  0.7071067812,            0,             0],\n",
       "[           0,             0, 0.7071067812, -0.7071067812],\n",
       "[           0,             0, 0.7071067812,  0.7071067812],\n",
       "[0.7071067812, -0.7071067812,            0,             0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 454.517x284.278 with 1 Axes>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quantum Circuit with Hadamard and CX\n",
    "circ0 = QuantumCircuit(2, 2)\n",
    "circ0.h(0)\n",
    "circ0.cx(0, 1)\n",
    "\n",
    "get(circ0)\n",
    "\n",
    "circ0.measure([0, 1], [0, 1])\n",
    "\n",
    "circ0.draw('mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulator = Aer.get_backend('qasm_simulator')\n",
    "\n",
    "# For Mathematical Representation\n",
    "results = execute(circ0, simulator).result()\n",
    "\n",
    "# Count Results\n",
    "counts = results.get_counts(circ0)\n",
    "\n",
    "# Plot Histogram\n",
    "plot_histogram(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs of the four CBS for this circuit are the following\n",
    "\n",
    "Let matrix B represents the unitary of the circuit\n",
    "\n",
    "$$B |00\\rangle = \\frac{1}{\\sqrt{2}} (|00\\rangle + |11\\rangle)$$\n",
    "\n",
    "$$B |01\\rangle = \\frac{1}{\\sqrt{2}} (|00\\rangle - |11\\rangle)$$\n",
    "\n",
    "$$B |10\\rangle = \\frac{1}{\\sqrt{2}} (|01\\rangle + |10\\rangle)$$\n",
    "\n",
    "$$B |11\\rangle = \\frac{1}{\\sqrt{2}} (-|01\\rangle + |10\\rangle)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Hadamard Decomposition\n",
    "\n",
    "Decompose Hadamard into $R_X(\\phi)$ and $R_Y(\\theta)$ using gate decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hadamard\n",
    "\n",
    "$$H = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 1\\\\ 1 & -1  \\end{bmatrix}$$\n",
    "\n",
    "#### X-axis Rotation\n",
    "\n",
    "$$R_X( \\theta ) = \\begin{bmatrix} \\cos \\frac{\\theta}{2} & -i \\sin \\frac{\\theta}{2} \\\\ -i \\sin \\frac{\\theta}{2} & \\cos \\frac{\\theta}{2}  \\end{bmatrix}$$\n",
    "\n",
    "#### Y-axis Rotation\n",
    "\n",
    "$$R_Y( \\theta ) = \\begin{bmatrix} \\cos \\frac{\\theta}{2} & -\\sin \\frac{\\theta}{2} \\\\ \\sin \\frac{\\theta}{2} & \\cos \\frac{\\theta}{2}  \\end{bmatrix}$$\n",
    "\n",
    "The Hadamard decomposition is as followed:\n",
    "\n",
    "$$H = R_y(-\\frac{\\pi}{2})R_x(\\pi)$$\n",
    "\n",
    "Subsequently, the new circuit will looks like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# Implement a R_y, R_x circuit. Notice that the order is flipped\n",
    "circ = QuantumCircuit(2)\n",
    "circ.rx(np.pi, 0)\n",
    "circ.ry(-np.pi/2, 0)\n",
    "circ.cx(0, 1)\n",
    "\n",
    "circ.draw('mpl')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs of the four CBS for this circuit are the following\n",
    "\n",
    "Let matrix M represents the unitary of the circuit\n",
    "\n",
    "$$M |00\\rangle = \\frac{1}{\\sqrt{2}} (|00\\rangle + |11\\rangle)$$\n",
    "\n",
    "$$M |01\\rangle = \\frac{1}{\\sqrt{2}} (|00\\rangle - |11\\rangle)$$\n",
    "\n",
    "$$M |10\\rangle = \\frac{1}{\\sqrt{2}} (|01\\rangle + |10\\rangle)$$\n",
    "\n",
    "$$M |11\\rangle = \\frac{1}{\\sqrt{2}} (-|01\\rangle + |10\\rangle)$$\n",
    "\n",
    "This is the same as the normal Bell states circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Draw Neural Network\n",
    "\n",
    "The neural network for this aims to mimic the $R_y(-\\frac{\\pi}{2})R_x(\\pi)$ matrix complex\n",
    "\n",
    "When plugging in the correct value, the $-i$ factors on the $R_x(\\phi)$ eventually dissapear as a global phase. \n",
    "\n",
    "Because of this, we can frame the $R_x(\\phi)$ in a simpler form\n",
    "\n",
    "$$R_y(\\theta)R_x(\\phi)(X) = \\begin{bmatrix} \\cos \\frac{\\theta}{2} & -\\sin \\frac{\\theta}{2} \\\\ \\sin \\frac{\\theta}{2} & \\cos \\frac{\\theta}{2}  \\end{bmatrix} \\begin{bmatrix} \\cos \\frac{\\phi}{2} & \\sin \\frac{\\phi}{2} \\\\ \\sin \\frac{\\phi}{2} & \\cos \\frac{\\phi}{2}  \\end{bmatrix} \\begin{pmatrix} x_0 \\\\ x_1 \\end{pmatrix}$$\n",
    "\n",
    "The middle two matrices can be reduced into sum and difference formula\n",
    "\n",
    "$$R_y(\\theta)R_x(\\phi)(X) = \\begin{bmatrix} \\cos\\frac{\\phi + \\theta}{2} & \\sin\\frac{\\phi - \\theta}{2} \\\\ \\sin\\frac{\\phi + \\theta}{2} & \\cos\\frac{\\phi - \\theta}{2}\\end{bmatrix} \\begin{pmatrix} x_0 \\\\ x_1 \\end{pmatrix}$$\n",
    "\n",
    "This neural network has one layers, each layers with two nodes, and each nodes with two weights. We can set the weight matrix $W$ as follow:\n",
    "\n",
    "$$W = \\begin{bmatrix} \\cos\\frac{\\phi + \\theta}{2} & \\sin\\frac{\\phi - \\theta}{2} \\\\ \\sin\\frac{\\phi + \\theta}{2} & \\cos\\frac{\\phi - \\theta}{2}\\end{bmatrix} = \\begin{bmatrix} \\alpha & \\beta \\\\ \\gamma & \\delta \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_nn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Each nodes in the hidden layer and the output layer contains two weights\n",
    "\n",
    "The square bracket index represents the layer the node is in.\n",
    "\n",
    "The curly bracket index represents the position of the node in its respective layer.\n",
    "\n",
    "###### Output Layer\n",
    "\n",
    "$$y_0 = W^{[1]\\{0\\}} \\cdot X$$\n",
    "\n",
    "Where $W^{[1]\\{0\\}} = \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix}$, $\\alpha$ and $\\beta$ are parameters\n",
    "\n",
    "From this, we can see that the output equals\n",
    "\n",
    "$$ y_0 = \\alpha x_0 + \\beta x_1 $$\n",
    "\n",
    "Also,\n",
    "\n",
    "$$y_1 = \\gamma x_0 + \\delta x_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create data\n",
    "\n",
    "There are two approachs to doing this: \n",
    "\n",
    "1. Create a small number of training examples so the algorithm intentionally overfits the data ( < 10 $(X, y)$ pairs)\n",
    "2. Create a large number of training examples so the algorithm can be more robust ( > 1000 $(X, y)$ pairs)\n",
    "\n",
    "Both approachs will be attempted and the better of the two will be chosen\n",
    "\n",
    "Steps to creating data:\n",
    "\n",
    "1. Randomize a 2-d real unit vectors, this is $X$ (for this purpose, ignore complex number)\n",
    "2. Using the known matrix, take the product of the two, this is $y$\n",
    "3. Repeat the process $n$ times as deemed necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Vector Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```python\n",
    "M_correct = H\n",
    "\n",
    "### X-values\n",
    "\n",
    "# Create a random 2-d real vector\n",
    "x = np.random.rand(2, 1)\n",
    "\n",
    "# Normalize v\n",
    "x /= np.linalg.norm(x)\n",
    "\n",
    "### Y-values\n",
    "\n",
    "# Matrix Multiplication\n",
    "y = np.dot(M_correct, x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Vectors Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(M_correct, size = 500, train_split = 0.7, seed = 0):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Create two matrices of specified length consisting of 2-d vectors representing\n",
    "    a train and a test examples\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    M_correct: ndarray\n",
    "        Matrix that transform the input to the correct output\n",
    "    size: int\n",
    "        Number of examples (train & test) in the dataset\n",
    "    train_split: float\n",
    "        Proportion of the train data to the total dataset\n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_train: ndarray\n",
    "        Matrix containing the x-values of all the training examples\n",
    "    y_train: ndarray\n",
    "        Matrix containing the y-values of all the training examples\n",
    "    X_test: ndarray\n",
    "        Matrix containing the x-values of all the testing examples\n",
    "    y_test: ndarray\n",
    "        Matrix containing the y-values of all the testing examples\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Manage defaults for 'size'\n",
    "    if size == 'small':\n",
    "        size = 10\n",
    "    elif size == 'large':\n",
    "        size = 1000\n",
    "    \n",
    "    ### X-values\n",
    "    \n",
    "    # Create a random matrix of 2-d real vectors as columns\n",
    "    X = np.random.rand (2, size)\n",
    "    \n",
    "    # Normalize v by column axix\n",
    "    X /= np.linalg.norm(X, axis = 0)\n",
    "    \n",
    "    # Check for normalization\n",
    "    assert np.round(np.sum(np.power(X, 2)), 10) == size, 'X is not normalized'\n",
    "    \n",
    "    # Check for correct shape\n",
    "    assert X.shape == (2, size), 'the shape of X is not correct'\n",
    "    \n",
    "    ### Y-values\n",
    "    \n",
    "    # Matrix Multiplication\n",
    "    y = np.dot(M_correct, X)\n",
    "    \n",
    "    # Check for unitary\n",
    "    assert np.round(np.sum(np.dot(M_correct, M_correct.conj().T)), 10) == 2, 'M_correct is not unitary'\n",
    "    \n",
    "    # Check for correct shape\n",
    "    assert y.shape == (2, size), 'the shape of y is not correct'\n",
    "    \n",
    "    # Splitting the matrix into its corresponding groups\n",
    "    \n",
    "    X_train = X[:, :int(size*train_split)]\n",
    "    y_train = y[:, :int(size*train_split)]\n",
    "    \n",
    "    X_test = X[:, int(size*train_split):]\n",
    "    y_test = y[:, int(size*train_split):]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Initialization\n",
    "The $W$ matrix will be initialized to all zeros or randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(init_type = 'random', dim = (2, 2), seed = 0):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Initialize the W matrix of a specified size to all zeros or randomly\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    init_type: str\n",
    "        Specify whether W will be initialized to all zeros or randomly\n",
    "    shape: tuple\n",
    "        Specify shape of W\n",
    "    seed: int\n",
    "        Used for pseudo-randomly generate matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    W: ndarray\n",
    "        Initialized W matrix (all zeros or pseudo-random)\n",
    "        \n",
    "    \"\"\"\n",
    "      \n",
    "    # Zero Initialization\n",
    "    if init_type == 'zeros':\n",
    "        \n",
    "        W = np.zeros(dim)  \n",
    "        \n",
    "    # Random Initialization\n",
    "    else:\n",
    "        # Keep the values consistent\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        W = np.random.rand(*dim)\n",
    "\n",
    "    # Check for W shape\n",
    "    assert W.shape == dim, 'shape of W is not correct'\n",
    "        \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Forward Propagation\n",
    "\n",
    "Forward propagation passes the input into the neural network to predict an output value\n",
    "\n",
    "$$\\hat{y} = \\underbrace{\\begin{bmatrix} \\alpha & \\beta \\\\ \\gamma & \\delta \\end{bmatrix}}_{W} \\begin{pmatrix} x_0 \\\\ x_1 \\end{pmatrix}$$\n",
    "\n",
    "To run multiple training examples at the same time (stochastic), put the training examples into a matrix a multiply it with the weight matrix. The index of training examples will be represented by the superscript inside the parentheses.\n",
    "\n",
    "$$W(X) \\ = \\ W \\begin{pmatrix} x_0^{(0)} & x_0^{(1)} ... & \\ x_0^{(n)} \\\\ x_1^{(0)} & x_1^{(1)} ... & \\ x_1^{(n)} \\end{pmatrix} \\ = \\ \\begin{pmatrix} \\hat{y}^{(0)} & \\hat{y}^{(1)} & ... & \\hat{y}^{(n)}\\end{pmatrix}$$\n",
    "\n",
    "There are two ways of running forward propagation:\n",
    "\n",
    "1. Simulate the circuit using 'qasm_simulator' \n",
    "\n",
    "    - This is less practical because it requires taking the square root of the count probability to get back to the weights.\n",
    "    \n",
    "    \n",
    "2. Calculate the output value using directly using linear algebra\n",
    "\n",
    "    - Forward prop will be calculated using this method at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(W, X):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Perform the forward propagation step\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    W: ndarray\n",
    "        Matrix of the weights in used\n",
    "    X: ndarray \n",
    "        Matrix of x-values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y_hat: ndarray\n",
    "        Predicted value to be used for back-prop\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    y_hat = np.dot(W, X)\n",
    "        \n",
    "    # Make sure the shape is consistent\n",
    "    assert X.shape == y_hat.shape, 'Shape is not consistent'\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Define Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is a regression problem, we will calculate the cost using Half Mean Squared Error (MSE), which we will define as\n",
    "\n",
    "$$J(W) = \\frac{1}{2N} \\sum^{N}_{i=1} \\underbrace{||y_i - \\hat{y}_i||^2}_{L(W)}$$\n",
    "\n",
    "where $N$ is the number of training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(y, y_hat):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Compute half mean-squared error cost\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y: ndarray\n",
    "        The true y value generated from a preset matrix (see step 4: Create Data)\n",
    "    y_hat: ndarray\n",
    "        The predicted y value from a learned set of weights (see step 6: Forward Propagation)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J: float\n",
    "        Scalar matrix to indicate the overall performance of the current set of weights\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # N is number of training examples\n",
    "    N = y.shape[1]\n",
    "    \n",
    "    # Compute cost function\n",
    "    J = 1/(2*N) * np.sum(np.linalg.norm(y - y_hat)**2)\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding the matrices from above (only caring about the subscript, we have)\n",
    "\n",
    "$$\\hat{y} = \\begin{bmatrix} \\cos\\frac{\\phi + \\theta}{2} & \\sin\\frac{\\phi - \\theta}{2} \\\\ \\sin\\frac{\\phi + \\theta}{2} & \\cos\\frac{\\phi - \\theta}{2}\\end{bmatrix} \\begin{pmatrix} x_0 \\\\ x_1 \\end{pmatrix} = \\underbrace{\\begin{bmatrix} \\alpha & \\beta \\\\ \\gamma & \\delta \\end{bmatrix}}_{W} \\begin{pmatrix} x_0 \\\\ x_1 \\end{pmatrix} = \\begin{pmatrix} \\alpha x_0 + \\beta x_1 \\\\ \\gamma x_0 + \\delta x_1 \\end{pmatrix}$$\n",
    "\n",
    "To find the local minimum, update the parameters opposite of the direction of steepest descent. Backpropagation calculates derivative of the cost function with respect to each of the weights.\n",
    "\n",
    "From above\n",
    "\n",
    "$$J(W) = \\frac{1}{2N} \\sum^{N}_{i=1} \\underbrace{||y_i - \\hat{y}_i||^2}_{L(W)}$$\n",
    "\n",
    "The cost function can be written as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(W) = \\frac{1}{2N} \\sum^{N}_{i=1} \\begin{Vmatrix} y_0 - (\\alpha x_0 + \\beta x_1) \\\\ y_1 - (\\gamma x_0 + \\delta x_1) \\end{Vmatrix}^2 = \\frac{1}{2N} \\sum^{N}_{i=1} \\overbrace{{\\underbrace{[(\\alpha x_0 + \\beta x_1 - y_0]}_{L_0}}^2 + {\\underbrace{[\\gamma x_0 + \\delta x_1 - y_1]}_{L_1}}^2}^{L(W)}$$\n",
    "\n",
    "In this case, the loss function $L(W)$ can be expressed as the sum of $L_0$ and $L_1$\n",
    "\n",
    "$$L(W) = (L_0)^2 + (L_1)^2$$\n",
    "\n",
    "Taking partial derivative with respect to a general weight ($w$)\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w}J(W) = \\frac{1}{2N} \\sum^{N}_{i=1} \\frac{\\partial}{\\partial w} L(W) = \\frac{1}{2N} \\sum^{N}_{i=1} \\frac{\\partial}{\\partial w} [(L_0)^2 + (L_1)^2]$$ \n",
    "\n",
    "A $2$ can be factored out after taking the derivative of $(L_0)^2$ and $(L_1)^2$\n",
    "\n",
    "$$= \\frac{1}{N} \\sum^{N}_{i=1} \\begin{pmatrix} L_0 \\frac{\\partial L_0}{\\partial w} + L_1 \\frac{\\partial L_1}{\\partial w} \\end{pmatrix}$$ \n",
    "\n",
    "Put everything together, the cost function derivatives with respect to $\\alpha, \\beta, \\gamma$ and $\\delta$ are\n",
    "\n",
    "###### Alpha\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\alpha} = \\frac{1}{N} \\sum^{N}_{i=1} L_0 x_0$$\n",
    "\n",
    "###### Beta\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\beta} = \\frac{1}{N} \\sum^{N}_{i=1} L_0 x_1$$\n",
    "\n",
    "###### Gamma\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\gamma} = \\frac{1}{N} \\sum^{N}_{i=1} L_1 x_0$$\n",
    "\n",
    "###### Delta\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\delta} = \\frac{1}{N} \\sum^{N}_{i=1} L_1 x_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement these four calculations in code with multiple training examples, \n",
    "\n",
    "1. Calculate the difference between all the $y$'s and $\\hat{y}$'s\n",
    "\n",
    "$$\\text{diff} = \\begin{pmatrix} y_0 - (\\alpha x_0 + \\beta x_1) \\\\ y_1 - (\\gamma x_0 + \\delta x_1) \\end{pmatrix} = y - \\hat{y} = \\begin{pmatrix} L_0 \\\\ L_1 \\end{pmatrix}$$\n",
    "\n",
    "2. Elongate this 'diff' matrix & the $X$ matrix as follow and perform element-wise multiplication\n",
    "\n",
    "$$\\text{elem_prod} = \\begin{bmatrix} \\begin{pmatrix} L_0 \\\\ L_0 \\end{pmatrix} \\\\ \\begin{pmatrix} L_1 \\\\ L_1  \\end{pmatrix} \\end{bmatrix} * \\begin{bmatrix} \\begin{pmatrix} x_0 \\\\ x_1 \\end{pmatrix} \\\\ \\begin{pmatrix} x_0 \\\\ x_1 \\end{pmatrix} \\end{bmatrix} = \\begin{pmatrix} L_0 x_0 \\\\ L_0 x_1 \\\\ L_1 x_0 \\\\ L_1 x_1\\end{pmatrix}$$\n",
    "\n",
    "3. Recall that each each $x$ and $L$ contains multiple training examples. \n",
    "\n",
    "Take the average of the row of the matrix and unpack it to the appropriate derivative variable. Note that $\\frac{\\partial J}{\\partial w}$ turns into $\\partial w$\n",
    "\n",
    "$$J = \\frac{1}{N} \\begin{pmatrix} \\sum^{N}_{i=1} L_0 x_0 \\\\ \\sum^{N}_{i=1} L_0 x_1 \\\\ \\sum^{N}_{i=1} L_1 x_0 \\\\ \\sum^{N}_{i=1} L_1 x_1\\end{pmatrix} = \\begin{pmatrix} \\partial \\alpha \\\\ \\partial \\beta \\\\ \\partial \\gamma \\\\ \\partial \\delta \\end{pmatrix}$$\n",
    "\n",
    "4. Finally reshape the weights into a square matrix for the gradient descent step\n",
    "\n",
    "$$\\begin{pmatrix} \\partial \\alpha \\\\ \\partial \\beta \\\\ \\partial \\gamma \\\\ \\partial \\delta \\end{pmatrix} \\longrightarrow \\begin{bmatrix} \\partial \\alpha & \\partial \\beta \\\\ \\partial \\gamma & \\partial \\delta \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(y, y_hat, X):\n",
    "        \n",
    "    \"\"\"\n",
    "    Calculate derivatives of the cost function (see step 7: Define Cost Function)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y: ndarray\n",
    "        The true y value generated from a preset matrix (see step 4: Create Data)\n",
    "    y_hat: ndarray\n",
    "        The predicted y value from a learned set of weights (see step 6: Forward Propagation)\n",
    "    X: ndarray\n",
    "        The X value used to generate the 'y' value\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dJ: ndarray\n",
    "        Matrix of partial derivatives with respect to each of the weights\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Extract number of examples\n",
    "    N = y_hat.shape[1]\n",
    "\n",
    "    # Reshape X\n",
    "    X = np.reshape(X, (2, N))\n",
    "\n",
    "    # Calculate the y - y_hat\n",
    "    diff = y - y_hat\n",
    "\n",
    "    # Elongate the diff array with repeating elements\n",
    "    diff = np.repeat(diff, 2, axis = 0)\n",
    "\n",
    "    # Reshape\n",
    "    diff = np.reshape(diff, (4, N))\n",
    "\n",
    "    # Elongate the x array with alternating elements\n",
    "    X = np.vstack((X, X))\n",
    "\n",
    "    # Check to see if both array has the same shape\n",
    "\n",
    "    assert X.shape == diff.shape, 'Shape is not consistent'\n",
    "\n",
    "    # Element-wise multiplication between the diff matrix and the x matrix\n",
    "    elem_prod = diff * X\n",
    "\n",
    "    # Average the row of the matrix\n",
    "    dJ = (1/N) * np.sum(elem_prod, axis = 1)\n",
    "\n",
    "    # Reshape\n",
    "\n",
    "    dJ = np.reshape(dJ, (4, 1))\n",
    "\n",
    "    # Check J shape\n",
    "    assert dJ.shape == (4,1), 'Shape is not correct'\n",
    "\n",
    "    # Reshape J into a matrix\n",
    "    dJ = dJ.reshape(2, 2)\n",
    "\n",
    "    assert dJ.shape == (2, 2), 'Shape is not square'\n",
    "\n",
    "    return dJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Gradient Descent (GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general weight update with learning rate $\\rho$ is as follow\n",
    "\n",
    "$$w := w - \\rho \\frac{\\partial J}{\\partial w}$$\n",
    "\n",
    "$\\rho$ is defaulted as 0.001. However, once the implementation is sucessful, $\\rho$ can be varied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(weights, grad, learning_rate = 0.001):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Update the weights using gradients calculated from step 8: Backpropagation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    weights: ndarray\n",
    "        Matrix of weights used for training\n",
    "    grad: ndarray\n",
    "        Matrix of gradients calculated from step 8: Backpropagation\n",
    "    learning_rate: float\n",
    "        Determines the speed step size of the weights update\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    new_weights: ndarray\n",
    "        Matrix of weights after updated\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    new_weights = weights - learning_rate * grad\n",
    "    \n",
    "    assert weights.shape == new_weights.shape\n",
    "    \n",
    "    assert new_weights.shape == (2, 2)\n",
    "    \n",
    "    return new_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Check for Correct Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checklist:\n",
    "\n",
    "    1. Create train/test data\n",
    "    2. Initialization\n",
    "    3. Forward Propagation\n",
    "    4. Define Cost Function\n",
    "    5. Back Propagation\n",
    "    6. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Create train/test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the following:\n",
    "\n",
    "1. Correct shape for both train and test set from 'size' and 'train_split'\n",
    "2. The y matrix is the output of the X matrix with the M_correct\n",
    "\n",
    "Run the code below to check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for i in range(10_000):\n",
    "    \n",
    "    # Size of dataset\n",
    "    n = np.random.randint(1, 10_000)\n",
    "    \n",
    "    # Train split\n",
    "    split = np.random.uniform()\n",
    "    \n",
    "    # Random unitary matrix\n",
    "    M = unitary_group.rvs(2)\n",
    "        \n",
    "    X_train, y_train, X_test, y_test = create_data(M, size = n, train_split = split)\n",
    "    \n",
    "    # Same shape for training pairs\n",
    "    assert X_train.shape == y_train.shape\n",
    "    \n",
    "    # Same shape for testing pairs\n",
    "    assert X_test.shape == y_test.shape\n",
    "    \n",
    "    # Correct no. of examples from 'size' and 'train_split'\n",
    "    assert X_train.shape[1] == int(n * split)\n",
    "    \n",
    "    # Everything adds up to 'size'\n",
    "    assert X_train.shape[1] + X_test.shape[1] == n\n",
    "    \n",
    "    # Check the output y is correct\n",
    "    assert np.round(np.sum(np.dot(M, X_train) - y_train), 10) == 0\n",
    "    \n",
    "    assert np.round(np.sum(np.dot(M, X_test) - y_test), 10) == 0\n",
    "    \n",
    "print('Everything is good')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for this is simple enough to just be inspect by eye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3. Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking by running through random examples\n",
    "\n",
    "Run the code below to check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "for i in range(10_000):\n",
    "\n",
    "    # Random weight matrix\n",
    "    W = np.random.rand(2, 2)\n",
    "\n",
    "    # Random X matrix\n",
    "    n = np.random.randint(1, 10_000)\n",
    "    X = np.random.rand(2, n)\n",
    "\n",
    "    # Run function\n",
    "    forward_prop(W, X, path = None)\n",
    "\n",
    "print('Everything is good')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4. Define Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be checked by comparing it to the 'mean_squared_error' within scikit learn\n",
    "\n",
    "Run the code below to check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for i in range(10_000):\n",
    "    \n",
    "    np.random.seed(i)\n",
    "\n",
    "    n = np.random.randint(1, 10_000)\n",
    "    y = np.random.rand(2, n)\n",
    "    y_hat = np.random.rand(2, n)\n",
    "\n",
    "    assert np.round(mean_squared_error(y, y_hat) - cost(y, y_hat), 15) == 0\n",
    "     \n",
    "print('Everything is good')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 10_000 random examples, it can be seen that the two function is equivalent, when the first dimension of the matrix is 2. However, when this dimension is not 2, the difference is drastic. In the end, the cost(y, y_hat) function will be used because it is significantly faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5. Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the following:\n",
    "\n",
    "1. Manually check if all the steps is implemented correctly\n",
    "\n",
    "2. Check that back_prop(y, y, X) = 0, 0, 0, 0\n",
    "\n",
    "3. Perform gradient checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This is the first step. The code preceding the assert statement is a step-by-step break down (non-vectorized) implementation of the back-prop module. It is manually check for small matrix size. The result is the same for the first 100_000 examples\n",
    "\n",
    "```python\n",
    "for i in range(100_000):\n",
    "    \n",
    "    np.random.seed(i)\n",
    "    \n",
    "    n = np.random.randint(1, 10_000)\n",
    "\n",
    "    X = np.random.rand(2, n)\n",
    "\n",
    "    x_0 = X[0]\n",
    "    x_0 = np.reshape(x_0, (1, n))\n",
    "\n",
    "    x_1 = X[1]\n",
    "    x_1 = np.reshape(x_1, (1, n))\n",
    "\n",
    "    np.random.seed(i+1)\n",
    "\n",
    "    y = np.random.rand(2, n)\n",
    "\n",
    "    np.random.seed(i+2)\n",
    "\n",
    "    y_hat = np.random.rand(2, n)\n",
    "\n",
    "    diff = y - y_hat\n",
    "\n",
    "    L_0 = diff[0]\n",
    "    L_0 = np.reshape(L_0, (1, n))\n",
    "\n",
    "    L_1 = diff[1]\n",
    "    L_1 = np.reshape(L_1, (1, n))\n",
    "\n",
    "    # Alpha\n",
    "    d_alpha = (1/n) * np.sum(L_0 * x_0)\n",
    "\n",
    "    # Beta\n",
    "    d_beta = (1/n) * np.sum(L_0 * x_1)\n",
    "\n",
    "    # Gamma\n",
    "    d_gamma = (1/n) * np.sum(L_1 * x_0)\n",
    "\n",
    "    # Delta\n",
    "    d_delta = (1/n) * np.sum(L_1 * x_1)\n",
    "\n",
    "    dJ = np.array([[d_alpha, d_beta], [d_gamma, d_delta]])\n",
    "    \n",
    "    assert np.sum(dJ - back_prop(y, y_hat, X)) == 0, 'not equal'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Check that back_prop(y, y, X) = 0, 0, 0, 0\n",
    "\n",
    "This is correct for 10_000 random examples\n",
    "\n",
    "```python\n",
    "for i in range(10_000):\n",
    "    n = np.random.randint(1, 10_000)\n",
    "    X_train, y_train, X_test, y_test = create_data(H, size = n, train_split = 1, seed = i)\n",
    "\n",
    "    assert np.all(back_prop(y_train, y_train, X_train) == np.zeros((2, 2)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Grad Check\n",
    "\n",
    "Run code below to compare 'back_prop' function and numerically calculated gradient\n",
    "\n",
    "The numerical derivative will be calculated using a two-sided derivative\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\lim_{\\varepsilon \\to 0} \\frac{J(w + \\varepsilon) - J(w - \\varepsilon)}{2 \\varepsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for i in range(10_000):\n",
    "    n = np.random.randint(1, 10_000)\n",
    "\n",
    "    X_train, y_train, _ , _   = create_data(H, size = n, train_split=1, seed = i)\n",
    "\n",
    "    W = initialize('random', dim = (2, 2), seed = i)\n",
    "\n",
    "    y_hat = forward_prop(W, X_train)\n",
    "\n",
    "    grad = back_prop(y_train, y_hat, X_train)\n",
    "\n",
    "    epsilon = 1e-7\n",
    "\n",
    "    # Create epsilon Matrix\n",
    "    ep_alpha = np.array([[epsilon, 0], [0, 0]])\n",
    "\n",
    "    ep_beta = np.array([[0, epsilon], [0, 0]])\n",
    "\n",
    "    ep_gamma = np.array([[0, 0], [epsilon, 0]])\n",
    "\n",
    "    ep_delta = np.array([[0, 0], [0, epsilon]])\n",
    "\n",
    "    # Alpha\n",
    "\n",
    "    # + epsilon on the alpha term\n",
    "    alpha_plus = W + ep_alpha\n",
    "    alpha_minus = W - ep_alpha\n",
    "\n",
    "    # Forward Prop\n",
    "    numerator = cost(y_train, forward_prop(alpha_plus, X_train)) - cost(y_train, forward_prop(alpha_minus, X_train))\n",
    "    denominator = 2 * epsilon\n",
    "\n",
    "    # Calculate Grad-appox\n",
    "    d_alpha_appox = np.mean(numerator / denominator) * 2\n",
    "\n",
    "    # Beta\n",
    "\n",
    "    # + epsilon on the beta term\n",
    "    beta_plus = W + ep_beta\n",
    "    beta_minus = W - ep_beta\n",
    "\n",
    "    # Forward Prop\n",
    "    numerator = cost(y_train, forward_prop(beta_plus, X_train)) - cost(y_train, forward_prop(beta_minus, X_train))\n",
    "    denominator = 2 * epsilon\n",
    "\n",
    "    # Calculate Grad-appox\n",
    "    d_beta_appox = np.mean(numerator / denominator) * 2\n",
    "\n",
    "    # Gamma\n",
    "\n",
    "    # + epsilon on the gamma term\n",
    "    gamma_plus = W + ep_gamma\n",
    "    gamma_minus = W - ep_gamma\n",
    "\n",
    "    # Forward Prop\n",
    "    numerator = cost(y_train, forward_prop(gamma_plus, X_train)) - cost(y_train, forward_prop(gamma_minus, X_train))\n",
    "    denominator = 2 * epsilon\n",
    "\n",
    "    # Calculate Grad-appox\n",
    "    d_gamma_appox = np.mean(numerator / denominator) * 2\n",
    "\n",
    "    # Delta\n",
    "\n",
    "    # + epsilon on the delta term\n",
    "    delta_plus = W + ep_delta\n",
    "    delta_minus = W - ep_delta\n",
    "\n",
    "    # Forward Prop\n",
    "    numerator = cost(y_train, forward_prop(delta_plus, X_train)) - cost(y_train, forward_prop(delta_minus, X_train))\n",
    "    denominator = 2 * epsilon\n",
    "\n",
    "    # Calculate Grad-appox\n",
    "    d_delta_appox = np.mean(numerator / denominator) * 2\n",
    "\n",
    "    # Put all the numerical gradient into a matrix\n",
    "    grad_appox = np.array([[d_alpha_appox, d_beta_appox], [d_gamma_appox, d_delta_appox ]])\n",
    "\n",
    "    assert np.round(np.linalg.norm(grad_appox - grad * -2), 7) == 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing grad check, it is found that the current 'back_prop' method generates a gradient matrix that is a factor of -2 away from the grad check value. \n",
    "\n",
    "The L2 norm between 'grad' (function) and 'grad_appox' (numerical) is zero, when round to 7 decimal places for 10_000 random examples. \n",
    "\n",
    "This suggests a certain correctness to the implementation of both system.\n",
    "\n",
    "It is more likely that the problem lies in the 'back_prop' function since there is inherently more calculation in that step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 6. Gradient Descent\n",
    "\n",
    "The code for this is simple enough to just be inspect by eye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is checked, the training can start. Implement the code by the following order\n",
    "\n",
    "    1. Create train/test data (X_train, y_train, X_test, y_test)\n",
    "    2. Initialize weights (W)\n",
    "    \n",
    "    Loop\n",
    "    \n",
    "    3. Forward Propagation (y_hat)\n",
    "    4. Define Cost Function (L)\n",
    "    5. Back Propagation (grad) * -2\n",
    "    6. Gradient Descent (W)\n",
    "    \n",
    "    Finally\n",
    "    \n",
    "    7. Normalized the matrix\n",
    "    \n",
    "The dataset is created before the model is trained\n",
    "The input of the 'train' method are\n",
    "    1. X-values\n",
    "    2. y-values\n",
    "    3. Weight initialization type ( = None ('random') )\n",
    "    4. Number of iterations (= 50)\n",
    "    5. Learning rate (= 0.1)\n",
    "    6. Loss / mse print frequency ( = 10)\n",
    "    7. Random seed ( = 0)\n",
    "    8. Print weights ( = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, init_weights = 'random', epoch = 50, \n",
    "          learning_rate = 0.1, print_frequency = 10, seed = 0, print_weights = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Train model from (X, y) pairs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ndarray\n",
    "        X-values for training\n",
    "    y: ndarray\n",
    "        y-values for the corresponding X-values\n",
    "    init_weights: str\n",
    "        'random' or 'zeros'\n",
    "    epoch: int\n",
    "        Number of weights updates through forward / backpropagation\n",
    "    learning_rate: float\n",
    "        Step size for weights update\n",
    "    print_frequency: int\n",
    "        Loss / mse print frequency\n",
    "    seed: int\n",
    "        Used for pseudo-randomly generate matrix\n",
    "    print_weights: bool\n",
    "        Show weights at the end for visual comparison\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    W: ndarray\n",
    "        Final weights after training\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Weights initialization\n",
    "    if type(init_weights) == str:\n",
    "        W = initialize(init_type = init_weights, seed = seed)\n",
    "    else:\n",
    "        W = init_weights\n",
    "\n",
    "    print('Training: \\n')\n",
    "    \n",
    "    for i in range(epoch):\n",
    "\n",
    "        # Foward Propagation\n",
    "        y_hat = forward_prop(W, X)\n",
    "\n",
    "        # Compute cost\n",
    "        L = cost(y, y_hat)\n",
    "        \n",
    "        # Compute mse\n",
    "        mse = cost(W, M_correct)\n",
    "\n",
    "        # Print loss and mse\n",
    "        if i % print_frequency == 0:\n",
    "            print(f'{i}th iteration, loss = {np.round(L, 6)}, mse = {np.round(mse, 6)}')\n",
    "\n",
    "        # Backpropagation\n",
    "        grad = back_prop(y, y_hat, X) * -2 # Multiply by a factor of -2 from above\n",
    "        \n",
    "        # Gradient Descent (The bigger the learning rate, the faster the model trains <= 1)\n",
    "        W = gradient_descent(W, grad, learning_rate = learning_rate) \n",
    "        \n",
    "    # Normalized W matrix\n",
    "    \n",
    "    W /= np.linalg.norm(W, axis = 0)\n",
    "        \n",
    "    # Print weights\n",
    "    if print_weights:\n",
    "        print('\\n')\n",
    "        print('Final Weights')\n",
    "        view(W)\n",
    "        print('\\n')\n",
    "        print(f'MSE: {mse}')\n",
    "        \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the weights of the created test set. The metrics here is 'mse'.\n",
    "\n",
    "The parameters for this functions are:\n",
    "\n",
    "    1. Trained weights\n",
    "    2. X-values test set\n",
    "    3. y-values test set\n",
    "\n",
    "The function will return a 'mse' metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(trained_weights, X, y):\n",
    "    \n",
    "    \"\"\"\n",
    "    Test model from (X, y) pairs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    trained_weights: ndarray\n",
    "        Weights from previously trained model\n",
    "    X: ndarray\n",
    "        X-values for testing\n",
    "    y: ndarray\n",
    "        y-values for the corresponding X-values\n",
    "        \n",
    "    \"\"\"\n",
    "    print('Evaluation: \\n')\n",
    "    \n",
    "    # Foward Propagation\n",
    "    y_hat = forward_prop(trained_weights, X)\n",
    "        \n",
    "    # Compute mse\n",
    "    mse = cost(y, y_hat)\n",
    "\n",
    "    print(f'MSE: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13: Full Optimization Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Matrix:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.707106781186547 & 0.707106781186547\\\\0.707106781186547 & -0.707106781186547\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0.707106781186547,  0.707106781186547],\n",
       "[0.707106781186547, -0.707106781186547]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training: \n",
      "\n",
      "0th iteration, loss = 0.27635, mse = 0.400872\n",
      "10th iteration, loss = 0.02229, mse = 0.032334\n",
      "20th iteration, loss = 0.001798, mse = 0.002608\n",
      "30th iteration, loss = 0.000145, mse = 0.00021\n",
      "40th iteration, loss = 1.2e-05, mse = 1.7e-05\n",
      "50th iteration, loss = 1e-06, mse = 1e-06\n",
      "60th iteration, loss = 0.0, mse = 0.0\n",
      "70th iteration, loss = 0.0, mse = 0.0\n",
      "80th iteration, loss = 0.0, mse = 0.0\n",
      "90th iteration, loss = 0.0, mse = 0.0\n",
      "\n",
      "\n",
      "Final Weights\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.707106689092746 & 0.707108932164054\\\\0.707106873280337 & -0.707104630202498\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0.707106689092746,  0.707108932164054],\n",
       "[0.707106873280337, -0.707104630202498]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MSE: 6.010073833467673e-12\n"
     ]
    }
   ],
   "source": [
    "# Define correct matrix\n",
    "M_correct = H\n",
    "\n",
    "print('Correct Matrix:')\n",
    "view(M_correct)\n",
    "print('\\n')\n",
    "\n",
    "# Create data\n",
    "X_train, y_train, X_test, y_test = create_data(M_correct, size = 20, train_split = 0.7)\n",
    "\n",
    "# Train model\n",
    "model_weights = train(X_train, y_train, init_weights = 'random', epoch = 100, learning_rate = 1, print_frequency=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14: Sampling on Quantum Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole purpose of step 14: Sampling on Quantum Circuit is to transform the weights from outputted from gradient descent to the correct angles on the $R_y$ and $R_x$ gates\n",
    "\n",
    "The current implementation has 3 steps:\n",
    "\n",
    "$$\\text{1. Unitary Transform by Gram-Schmidt}$$\n",
    "\n",
    "$$\\downarrow$$\n",
    "\n",
    "$$\\text{2. Solve for $\\phi, \\theta$ } \\begin{cases} \\text{Perform 3rd Quadrant Inverse} \\\\ \\text{Find the odd-one-out element} \\end{cases}$$\n",
    "\n",
    "$$\\downarrow$$\n",
    "\n",
    "$$\\text{3. Apply the correct gates, and sample}$$\n",
    "\n",
    "Step $1$ and $3$ uses the most efficient implementation available.\n",
    "\n",
    "Any improvements that can be made will be made to step $2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Transforming Weights to Unitary Form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample qubits on a quantum circuit, the weights has to be in the form of a unitary matrix. \n",
    "\n",
    "This can be acheived by applying the Gram-Schmidt process to the matrix. The resulting orthonormal matrix is unitary in real numbers.\n",
    "\n",
    "To create an orthonormal basis for the set of vectors $\\{v_1, v_2\\}$,\n",
    "$$u_1 = \\frac{v_1}{||v_1||}\\$$\n",
    "\n",
    "$$y_2 = v_2 - (v_2 \\cdot u_1) \\ u_1$$\n",
    "\n",
    "$$u_2 = \\frac{y_2}{||y_2||}$$\n",
    "\n",
    "At the end, we have\n",
    "\n",
    "$$\\{v_1, v_2\\} \\longrightarrow \\{u_1, u_2\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gram-Schmidt for 2D real vector sets\n",
    "\n",
    "def gs_2d_real(M):\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform the Gram-Scmidt process on 2D real vectors\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    M: ndarray\n",
    "        Real 2D Matrix\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    M_ortho: ndarray\n",
    "        Orthonormal matrix\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split into individual components\n",
    "    v1 = M[:, 0].reshape(2, 1)\n",
    "    v2 = M[:, 1].reshape(2, 1)\n",
    "\n",
    "    # Normalize\n",
    "    u1 = v1 / np.linalg.norm(v1)\n",
    "\n",
    "    # Calculate orthogonal vector\n",
    "    y2 = v2 - np.dot(v2.T, u1) * u1\n",
    "\n",
    "    # Normalize\n",
    "    u2 = y2 / np.linalg.norm(y2)\n",
    "\n",
    "    # Put back to a matrix\n",
    "    ortho_M = np.hstack((u1, u2))\n",
    "\n",
    "    # Check for unitary\n",
    "    assert np.round(np.sum(np.dot(ortho_M, ortho_M.conj().T)), 10) == 2, 'ortho_M is not unitary'\n",
    "    \n",
    "    return ortho_M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the weights as unitary matrix, we can feed it into the circuit to sample qubits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 538.128x284.278 with 1 Axes>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Orthonormalize the weights\n",
    "U = gs_2d_real(model_weights)\n",
    "\n",
    "# Initialize Quantum Circuit\n",
    "circ1 = QuantumCircuit(2, 2)\n",
    "\n",
    "# Apply weights matrix as unitary gate\n",
    "circ1.unitary(U, 0)\n",
    "\n",
    "# Apply CX\n",
    "circ1.cx(0, 1)\n",
    "\n",
    "# Measure\n",
    "circ1.measure([0, 1], [0, 1])\n",
    "\n",
    "# Draw Circuit\n",
    "circ1.draw('mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulator = Aer.get_backend('qasm_simulator')\n",
    "\n",
    "# For Mathematical Representation\n",
    "results = execute(circ1, simulator).result()\n",
    "\n",
    "# Count Results\n",
    "counts = results.get_counts(circ1)\n",
    "\n",
    "# Plot Histogram\n",
    "plot_histogram(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Solving for $\\phi$ and $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Performing 3rd Quadrant Inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the problem requires that the circuit being built using only $R_x$ and $R_y$ gates. This means that we have to convert the matrix back to its angular form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$W = \\begin{bmatrix} \\alpha & \\beta \\\\ \\gamma & \\delta \\end{bmatrix} = \\begin{bmatrix} \\cos\\frac{\\phi + \\theta}{2} & \\sin\\frac{\\phi - \\theta}{2} \\\\ \\sin\\frac{\\phi + \\theta}{2} & \\cos\\frac{\\phi - \\theta}{2}\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equate one element from one matrix to another and taking the trigonometric inverse, we are left with four sets of equations\n",
    "\n",
    "$$\\frac{\\phi + \\theta}{2} = \\cos^{-1}\\alpha$$\n",
    "\n",
    "$$\\frac{\\phi + \\theta}{2} = \\sin^{-1}\\gamma$$\n",
    "\n",
    "$$\\frac{\\phi - \\theta}{2} = \\sin^{-1}\\beta$$\n",
    "\n",
    "$$\\frac{\\phi - \\theta}{2} = \\cos^{-1}\\delta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, due to the cyclical nature of the sinusoidal functions, we also have to following\n",
    "\n",
    "$$\\frac{\\phi + \\theta}{2} = -\\cos^{-1}\\alpha$$\n",
    "\n",
    "$$\\frac{\\phi + \\theta}{2} = \\pi-\\sin^{-1}\\gamma$$\n",
    "\n",
    "$$\\frac{\\phi - \\theta}{2} = \\pi-\\sin^{-1}\\beta$$\n",
    "\n",
    "$$\\frac{\\phi - \\theta}{2} = -\\cos^{-1}\\delta$$\n",
    "\n",
    "The system of equations might not look consistent because the $\\cos^{-1}$ return output in the $1st$ and $2nd$ quadrant, whereas $\\sin^{-1}$ return output in the $1st$ and $4th$ quadrant. \n",
    "f\n",
    "In the unit circle, the $\\sin$ & $\\cos$ function is the same only in the $1st$ and $3rd$ quadrant. This means that \n",
    "\n",
    "$$\\cos^{-1}(x) \\ \\& \\ \\sin^{-1}(x), \\ \\forall \\ x > 0 \\ \\longrightarrow \\ 1st \\text{ quadrant}$$\n",
    "\n",
    "$$\\cos^{-1}(x) \\ \\& \\ \\sin^{-1}(x), \\ \\forall \\ x < 0 \\ \\longrightarrow \\ 3rd \\text{ quadrant}$$\n",
    "\n",
    "We can create the following piece-wise function\n",
    "\n",
    "$$\\cos^{-1}_\\text{q3}(x) = \\begin{cases} \\cos^{-1} (x) \\ \\text{if } x \\geq 0 \\\\ 2 \\pi -\\cos^{-1} (x) \\ \\text{if } x < 0\\end{cases}$$\n",
    "\n",
    "$$\\sin^{-1}_\\text{q3}(x) = \\begin{cases} \\sin^{-1} (x) \\ \\text{if } x \\geq 0 \\\\ \\pi -\\sin^{-1} (x) \\ \\text{if } x < 0\\end{cases}$$\n",
    "\n",
    "The '$q3$' here just emphasizes the fact that the inverse function returns $\\theta$ value in the $3rd$ quadrant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Inverse Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "acos_q3 = lambda x: np.arccos(x) if x >= 0 else 2*np.pi - np.arccos(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Inverse Sine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_q3 = lambda x: np.arcsin(x) if x >= 0 else np.pi - np.arcsin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Full Inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_q3(M):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return a matrix with the appropriate 3rd quadrant inverse of each element\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    M: ndarray\n",
    "        Orthonormal matrix\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    M_inverse: ndarray\n",
    "        Matrix with the appropriate 3rd quadrant inverse\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the components\n",
    "    alpha = M[0, 0]\n",
    "    beta = M[0, 1]\n",
    "    gamma = M[1, 0]\n",
    "    delta = M[1, 1]\n",
    "\n",
    "    # Perform inverse trigs on each components\n",
    "    alpha = acos_q3(alpha)\n",
    "    beta = asin_q3(beta)\n",
    "    gamma = asin_q3(gamma)\n",
    "    delta = acos_q3(delta)\n",
    "    \n",
    "    # Check for uniqueness\n",
    "    assert (np.round(alpha - gamma, 10) == 0) ^ (np.round(beta - delta, 10) == 0), 'Not unique'\n",
    "    \n",
    "    M_inverse = np.array([[alpha, beta], [gamma, delta]])\n",
    "    \n",
    "    return M_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_inverse = inverse_q3(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creating a consistent system of equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we can see that the odd one out element $M_\\text{trig}$ is the element with different sign as the others in $M_\\text{orthogonal}$\n",
    "\n",
    "Take the examples from $M_\\text{trig}$ above, in a general form, we have\n",
    "\n",
    "$$M_\\text{ trig} = \\begin{bmatrix} \\alpha & \\alpha \\\\ \\alpha & w \\end{bmatrix}$$\n",
    "\n",
    "$$\\frac{\\phi + \\theta}{2} = \\begin{Bmatrix}  \\alpha \\\\ \\alpha \\end{Bmatrix} \\rightarrow \\text{ consistent}$$\n",
    "\n",
    "$$\\frac{\\phi - \\theta}{2} = \\begin{Bmatrix}  \\alpha \\\\ w \\end{Bmatrix} \\rightarrow \\text{ inconsistent!?}$$\n",
    "\n",
    "Let's assume that both sets of equation have the same value.\n",
    "\n",
    "$$\\frac{\\phi + \\theta}{2} = \\frac{\\phi - \\theta}{2} = \\alpha \\tag{Assumption}$$\n",
    "\n",
    "Solving this, the results are\n",
    "\n",
    "$$\\phi = 2\\alpha$$\n",
    "\n",
    "$$\\theta = 0$$\n",
    "\n",
    "Plugging the value back into the original rotation matrix, we have\n",
    "\n",
    "$$\\begin{pmatrix}\\phi = 2\\alpha \\\\ \\theta = 0 \\end{pmatrix} \\longrightarrow \\begin{bmatrix} \\cos\\frac{\\phi + \\theta}{2} & \\sin\\frac{\\phi - \\theta}{2} \\\\ \\sin\\frac{\\phi + \\theta}{2} & \\cos\\frac{\\phi - \\theta}{2}\\end{bmatrix} = \\underbrace{\\begin{bmatrix} \\cos\\phi & \\sin\\phi \\\\ \\sin\\phi & \\cos\\phi\\end{bmatrix}}_{\\text{not unitary } \\forall \\ \\phi }$$\n",
    "\n",
    "It is clear the the above matrix is not unitary for all values of $\\phi$, and therefore the assumption yields a contradiction. \n",
    "\n",
    "This means that each set of equation have a different value. We can do this by searching for the odd-one-out\n",
    "\n",
    "###### Odd-One-Out Grid Search\n",
    "\n",
    "**Problem**: Given an $n \\times n$ grid with elements, we want to identify $w$ such that $w$ is unique and all other elements are identical.\n",
    "\n",
    "Here because we know that $n\\geq 2$, we can use the comparison and search method, which has $O(n^2)$ complexity using linear search.\n",
    "\n",
    "The quantum analog of this method uses Grover's search and has $O(\\sqrt{n})$.\n",
    "\n",
    "Compare the first two element in the grid called $a$ and $b$ respectively. \n",
    "\n",
    "- If $a = b$, search for $w \\neq a$ using linear search or Grover's search\n",
    "\n",
    "- Else if $a \\neq b$, compare the $a$ with a third element, $c$\n",
    "\n",
    "    - If $a = c$, we have found $w = b$ in $O(1)$\n",
    "    \n",
    "    - Else if $a \\neq c$, we also have found $w = a$ in $O(1)$\n",
    "\n",
    "There are $O(1)$ comparison steps, and $O(n^2)$ or $O(\\sqrt{n})$ search steps depending on the algorithm, which makes the overall complexity $O(n^2)$ or $O(\\sqrt{n})$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odd_search(M):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return the constant vector used for solving systems of equations\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    M: ndarray\n",
    "        (n, n) matrix of possible constants of the system of equations\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    constant_vec: ndarray\n",
    "        (n, 1) constant vector \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    n = M.shape[0]\n",
    "\n",
    "    a = M[0, 0] # First element\n",
    "    b = M[1, 0] # Second element\n",
    "    c = M[0, 1] # Third element\n",
    "\n",
    "    if np.round(a - b, 10) == 0:\n",
    "\n",
    "        # Find the index of the element satisfying the conditions\n",
    "        coor = tuple(np.squeeze(np.where(M != a)))\n",
    "\n",
    "        # Build the vector\n",
    "        constant_vec = np.full(shape = (n, 1), fill_value = a)\n",
    "        constant_vec[coor[1], 0] = M[coor]\n",
    "\n",
    "    # Comparing with a third element\n",
    "    elif np.round(a - c, 10) == 0:\n",
    "\n",
    "        # Build the vector\n",
    "        constant_vec = np.full(shape = (n, 1), fill_value = a)\n",
    "        constant_vec[0] = b\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Build the vector\n",
    "        constant_vec = np.full(shape = (n, 1), fill_value = c)\n",
    "        constant_vec[0] = a\n",
    "\n",
    "    return constant_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70710678],\n",
       "       [-0.70710678]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_search(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we were able to have two set of equations with distince solutions in a vector\n",
    "\n",
    "$$\\frac{\\phi + \\theta}{2} = \\alpha$$\n",
    "\n",
    "$$\\frac{\\phi - \\theta}{2} = w$$\n",
    "\n",
    "Expand the rest of the variables into matrix form\n",
    "\n",
    "$$\\underbrace{\\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix}}_{M} \\ \\underbrace{\\begin{bmatrix} \\phi \\\\ \\theta \\end{bmatrix}}_{X} = 2 \\underbrace{\\begin{bmatrix} \\alpha \\\\ w \\end{bmatrix}}_{A}$$\n",
    "\n",
    "Solving for $\\phi, \\theta$ by taking the inverse of $M$ yields\n",
    "\n",
    "$$\\underbrace{\\begin{bmatrix} \\phi \\\\ \\theta \\end{bmatrix}}_{X} = \\underbrace{ \\frac{1}{2} \\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix} }_{M^{-1}} 2 \\underbrace{\\begin{bmatrix} \\alpha \\\\ w \\end{bmatrix}}_{A}$$\n",
    "\n",
    "Which reduces down to \n",
    "\n",
    "$$\\underbrace{\\begin{bmatrix} \\phi \\\\ \\theta \\end{bmatrix}}_{X} = \\underbrace{ \\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix} }_{M} \\underbrace{\\begin{bmatrix} \\alpha \\\\ w \\end{bmatrix}}_{A} = \\begin{bmatrix} \\alpha + w \\\\ \\alpha - w \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_sys(constant_vec):\n",
    "    \n",
    "    \"\"\"\n",
    "    Solve for phi, theta\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    constant_vec: ndarray\n",
    "        Vector of constants in the correct order\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    solution: ndarray\n",
    "        Vector of solutions correspond to phi, theta\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    variable_matrix = np.array([[1, 1], [1, -1]])\n",
    "    \n",
    "    solution = np.dot(variable_matrix, constant_vec).reshape(2, 1)\n",
    "    \n",
    "    return solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all this, we need the solution when plugged into $R_y$ and $R_x$ to be approximately equal to the original unitary matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10_000):\n",
    "\n",
    "    # Initialize random matrix\n",
    "    M_random = np.random.uniform(low = -1, high = 1, size = (2, 2))\n",
    "\n",
    "    # Orthonormalize it\n",
    "    M_ortho = gs_2d_real(M_random)\n",
    "\n",
    "    # Inverse Trig the components\n",
    "    M_inverse = inverse_q3(M_ortho)\n",
    "\n",
    "    # Get constant vector\n",
    "    constant_vec = odd_search(M_inverse)\n",
    "\n",
    "    # Solve for phi, theta\n",
    "    phi, theta = solve_sys(constant_vec)\n",
    "\n",
    "    # Rewrite Rx and Ry gate\n",
    "    Rx = lambda theta: np.array([[np.cos(theta/2), np.sin(theta/2)], [np.sin(theta/2), np.cos(theta/2)]])\n",
    "    Ry = lambda theta: np.array([[np.cos(theta/2), -np.sin(theta/2)], [np.sin(theta/2), np.cos(theta/2)]])\n",
    "\n",
    "    # Reapply back into rotational matrix\n",
    "    M_reconstruct = np.dot(Ry(phi).reshape(2, 2), Rx(theta).reshape(2, 2))\n",
    "\n",
    "    # The absolute value is used here because there can be different sign representation of orthonormal basis\n",
    "    # However, this does not affect the final probability\n",
    "    aae(np.abs(M_ortho), np.abs(M_reconstruct), decimal = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}-0.984754131848131 & 0.173952004323128\\\\-0.173952004323128 & -0.984754131848131\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[-0.984754131848131,  0.173952004323128],\n",
       "[-0.173952004323128, -0.984754131848131]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}-0.984754131848131 & 0.173952004323128\\\\-0.173952004323128 & -0.984754131848131\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[-0.984754131848131,  0.173952004323128],\n",
       "[-0.173952004323128, -0.984754131848131]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view(M_ortho)\n",
    "\n",
    "view(M_reconstruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put everything together, we can create an end-to-end circuit that takes the weights from gradient descent and implement the correct angle values for its rotation gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Implementation from Weights Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circ_from_weights(W, circ = 'init', measure = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a circuit using only Rx, Ry and Cx gates\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    W: ndarray\n",
    "        Weight Matrix     \n",
    "    circ: QuantumCircuit or 'yes':\n",
    "        Allows initialization of brand new circuit or reuse old circuit\n",
    "    measure: bool\n",
    "        Add measurement gates to circuit\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    circ: QuantumCircuit\n",
    "        Full circuit with the correct value for phi and theta in the Ry and Rx gates\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Orthonormalize W\n",
    "    M_ortho = gs_2d_real(W)\n",
    "\n",
    "    # Inverse Trig the components\n",
    "    M_inverse = inverse_q3(M_ortho)\n",
    "\n",
    "    # Get constant vector\n",
    "    constant_vec = odd_search(M_inverse)\n",
    "\n",
    "    # Solve for phi, theta\n",
    "    phi, theta = solve_sys(constant_vec)\n",
    "    \n",
    "    # Initialize circuit\n",
    "    \n",
    "    if circ == 'init':\n",
    "        circ = QuantumCircuit(1, 1)\n",
    "\n",
    "    # Apply Rx\n",
    "    circ.rx(theta[0], 0)\n",
    "\n",
    "    # Apply Ry\n",
    "    circ.ry(phi[0], 0)\n",
    "\n",
    "    # Measure\n",
    "    if measure:\n",
    "        circ.measure([0], [0])\n",
    "    \n",
    "    return circ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "circ2 = circ_from_weights(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulator = Aer.get_backend('qasm_simulator')\n",
    "\n",
    "# For Mathematical Representation\n",
    "results = execute(circ2, simulator).result()\n",
    "\n",
    "# Count Results\n",
    "counts = results.get_counts(circ2)\n",
    "\n",
    "# Plot Histogram\n",
    "plot_histogram(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all three implementations of the Bell States circuit (Step 1, Step 14.1, Step 14.3) yield the same results. \n",
    "\n",
    "We have now finished the main part of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation with Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classical analog of this is stochastic gradient descent, updating the weights for every example.\n",
    "\n",
    "To test the practicality of this on a quantum machine, first test this classically to see the number of iterations requires for the matrix to approaches its optimal value. \n",
    "\n",
    "This can be done by editing the 'train' function from above. A new argument called 'stochastic' will be added that will allow the option of training after every example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stoch0(X, y, init_weights = 'random', gd_method = 'batch', epoch = 50, \n",
    "          learning_rate = 0.1, print_frequency = 10, seed = 0, print_weights = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Train model from (X, y) pairs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ndarray\n",
    "        X-values for training\n",
    "    y: ndarray\n",
    "        y-values for the corresponding X-values\n",
    "    init_weights: str\n",
    "        'random' or 'zeros'\n",
    "    gd_method: str\n",
    "        'batch' or 'stochastic' (weights update after every trainign examples)\n",
    "    epoch: int\n",
    "        Number of weights updates through forward / backpropagation over entire dataset\n",
    "    learning_rate: float\n",
    "        Step size for weights update\n",
    "    print_frequency: int\n",
    "        Loss / mse print frequency\n",
    "    seed: int\n",
    "        Used for pseudo-randomly generate matrix\n",
    "    print_weights: bool\n",
    "        Show weights at the end for visual comparison\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    W: ndarray\n",
    "        Final weights after training\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Weights initialization\n",
    "    if type(init_weights) == str:\n",
    "        W = initialize(init_type = init_weights, seed = seed)\n",
    "    else:\n",
    "        W = init_weights\n",
    "\n",
    "    print('Training: \\n')\n",
    "    \n",
    "    if gd_method == 'stochastic':\n",
    "        \n",
    "        # Stochastic\n",
    "        \n",
    "        for n in range(epoch):\n",
    "            \n",
    "            # Iterate over training examples\n",
    "            for i in range(X.shape[1]):\n",
    "\n",
    "                # Reshape the individual training examples\n",
    "                X_stoch = X[:, i].reshape(2, 1) \n",
    "                y_stoch = y[:, i].reshape(2, 1)\n",
    "\n",
    "                # Foward Propagation\n",
    "                y_hat = forward_prop(W, X_stoch)\n",
    "\n",
    "                # Compute cost\n",
    "                L = cost(y_stoch, y_hat)\n",
    "\n",
    "                # Compute mse\n",
    "                mse = cost(W, M_correct)\n",
    "\n",
    "                # Backpropagation\n",
    "                grad = back_prop(y_stoch, y_hat, X_stoch) * -2 # Multiply by a factor of -2 from above\n",
    "\n",
    "                # Gradient Descent (The bigger the learning rate, the faster the model trains <= 1)\n",
    "                W = gradient_descent(W, grad, learning_rate = learning_rate) \n",
    "                \n",
    "            # Print loss and mse\n",
    "            if n % print_frequency == 0:\n",
    "                print(f'{n}th epoch, loss = {np.round(L, 6)}, mse = {np.round(mse, 6)}')\n",
    "                    \n",
    "    else:\n",
    "        \n",
    "        # Batch GD\n",
    "        \n",
    "        for n in range(epoch):\n",
    "\n",
    "            # Foward Propagation\n",
    "            y_hat = forward_prop(W, X)\n",
    "\n",
    "            # Compute cost\n",
    "            L = cost(y, y_hat)\n",
    "\n",
    "            # Compute mse\n",
    "            mse = cost(W, M_correct)\n",
    "\n",
    "            # Print loss and mse\n",
    "            if n % print_frequency == 0:\n",
    "                print(f'{n}th epoch, loss = {np.round(L, 6)}, mse = {np.round(mse, 6)}')\n",
    "\n",
    "            # Backpropagation\n",
    "            grad = back_prop(y, y_hat, X) * -2 # Multiply by a factor of -2 from above\n",
    "\n",
    "            # Gradient Descent (The bigger the learning rate, the faster the model trains <= 1)\n",
    "            W = gradient_descent(W, grad, learning_rate = learning_rate) \n",
    "        \n",
    "        \n",
    "    # Normalized W matrix\n",
    "    \n",
    "    W /= np.linalg.norm(W, axis = 0)\n",
    "        \n",
    "    # Print weights\n",
    "    if print_weights:\n",
    "        print('\\n')\n",
    "        print('Final Weights')\n",
    "        view(W)\n",
    "        print(f'MSE: {mse}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test whether stochastic GD or batch GD is better accuracy-wise (optimizing) and time-wise (satisfiscing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Matrix:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.707106781186547 & 0.707106781186547\\\\0.707106781186547 & -0.707106781186547\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0.707106781186547,  0.707106781186547],\n",
       "[0.707106781186547, -0.707106781186547]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training: \n",
      "\n",
      "0th epoch, loss = 0.742812, mse = 0.398883\n",
      "10th epoch, loss = 0.443207, mse = 0.365374\n",
      "20th epoch, loss = 0.275237, mse = 0.367223\n",
      "30th epoch, loss = 0.178193, mse = 0.381542\n",
      "40th epoch, loss = 0.120361, mse = 0.398795\n",
      "50th epoch, loss = 0.084804, mse = 0.415259\n",
      "60th epoch, loss = 0.062256, mse = 0.429691\n",
      "70th epoch, loss = 0.047526, mse = 0.441869\n",
      "80th epoch, loss = 0.037627, mse = 0.451956\n",
      "90th epoch, loss = 0.030799, mse = 0.460235\n",
      "\n",
      "\n",
      "Final Weights\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.997911636827881 & 0.338413209591806\\\\0.0645938471024811 & 0.940997608697159\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[ 0.997911636827881, 0.338413209591806],\n",
       "[0.0645938471024811, 0.940997608697159]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.46638347047789136\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define correct matrix\n",
    "M_correct = H\n",
    "\n",
    "print('Correct Matrix:')\n",
    "view(M_correct)\n",
    "print('\\n')\n",
    "\n",
    "# Create data\n",
    "X_train, y_train, X_test, y_test = create_data(M_correct, size = 3, train_split = 1)\n",
    "\n",
    "# Train model\n",
    "\n",
    "model_1 = train_stoch0(X_sim_set, y_train, init_weights = 'random', gd_method = 'stochastic', epoch = 100, learning_rate = 0.01\n",
    "                      , print_frequency=10, seed = 1)\n",
    "\n",
    "# model_2 = train_stoch(X_train, y_train, init_weights = 'random', gd_method = 'batch', epoch = 35, learning_rate = 1, print_frequency=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99999354,  0.96876136,  0.99940397],\n",
       "       [ 0.00359359,  0.24799479, -0.0345211 ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 9 training examples, the stochastic gradient descent performs well with 5 epochs. Batch gradient descent offers comparable accuracy with 40 epochs. However, if time is a constraint. Stochastic gradient descent far outperforms batch gradient descent in more than half the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the gradient descent method of the simulation. The steps to implement forward propagation with simulation is similar to the classical method with a few tweaks to adapt to the machinery used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the basic outline of the things involved:\n",
    "\n",
    "    1. Initialize the starting qubits to some special states using the $R_X$ & $R_Y$ gate at the disposal / Create X_train data\n",
    "\n",
    "    2. Initialize a random weight matrix by randomize the $\\theta$ & $\\phi$ arguments / Create y_train data\n",
    "\n",
    "    3. Run the states over the circuit\n",
    "\n",
    "    4. Measure the results, get the count distribution and infer back the amplitudes. \n",
    "\n",
    "This is done by square rooting the probability number. \n",
    "\n",
    "This means that the amplitude can only have positive arguments. \n",
    "\n",
    "Therefore, we have to choose the appropriate starting values that has positive output amplitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Initialize the starting qubits to some special separable states using the $R_X$ & $R_Y$ gate at the disposal / Create X_train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we cannot infer negative or complex phase, we have to resort to output value that is real and non-negative. Below are certain examples that are found that satisfies this property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal:\n",
    "\n",
    "$$|0\\rangle$$\n",
    "\n",
    "H(0):\n",
    "\n",
    "$$|0\\rangle, |1\\rangle$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this means is that the states is inputted and then the gate appropriate gate is applied to initialized it to some superposition. \n",
    "\n",
    "To implement this, we need \n",
    "\n",
    "1. A function is initialized the value to the correct superposition\n",
    "\n",
    "2. A function to calculate the expected output value of the given state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Qubit Initialization Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the circuit could only use $R_X$ and $R_Y$ gates, this means that the true input is always $|00\\rangle$ and any manipulation is done after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Basic Architecture\n",
    "\n",
    "def init_special(*init):\n",
    "    \n",
    "    \"\"\"\n",
    "    Initialized circuit using only Rx and Ry gates\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    init: tuple\n",
    "        Contains 'gate' and 'CBS'\n",
    "        \n",
    "    Returns:\n",
    "    circ: QuantumCircuit\n",
    "        Return initialized circuit using only RX and Ry gates\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    gate, CBS = init\n",
    "\n",
    "    circ = QuantumCircuit(1)\n",
    "    \n",
    "    ## CBS\n",
    "    if CBS == '1':\n",
    "        circ.rx(np.pi, 0)\n",
    "\n",
    "    ## Gate\n",
    "    ## pseudo-H gate\n",
    "    if gate == 'H':\n",
    "        circ.rx(np.pi, 0)\n",
    "        circ.ry(-np.pi/2, 0)\n",
    "        circ.barrier()\n",
    "\n",
    "    return circ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "circ = init_special('H', '1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to manually create the X_train portion of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create X_train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sim = [(None, '0'), ('H', '0'), ('H', '1')] # X set for the circuit\n",
    "X_sim_set = np.hstack((zero, H[:, 0].reshape(2, 1), H[:, 1].reshape(2, 1))) # X set for the linear algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.70710678,  0.70710678],\n",
       "       [ 0.        ,  0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sim_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.7071067812 & 0.7071067812\\\\-0.7071067812 & 0.7071067812\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[ 0.7071067812, 0.7071067812],\n",
       "[-0.7071067812, 0.7071067812]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "circ = init_special(*X_sim[2])\n",
    "\n",
    "get(circ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### 2. Initialize a random weight matrix by randomize the $\\theta$ & $\\phi$ arguments\n",
    "\n",
    "We already satisfy this with the 'circ_from_weights' function from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize random phi and theta\n",
    "W = np.random.rand(2, 2)\n",
    "\n",
    "circ = circ_from_weights(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create y_train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this by running the above states through the full circuit and get the statevector. This steps doubles as checking. This is because the rotation gates might add a global phase to the output but we know that it is trivial.\n",
    "\n",
    "We can create two parts to the circuit (init and weights) and concatenate them at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_sim = np.zeros((2, 1))\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    # Init Part\n",
    "    circ_init = init_special(*X_sim[i])\n",
    "    circ_init.draw('mpl')\n",
    "\n",
    "    # Weight Part\n",
    "    circ_weights = QuantumCircuit(1)\n",
    "    circ_weights.rx(np.pi, 0)\n",
    "    circ_weights.ry(-np.pi/2, 0)\n",
    "\n",
    "    circ_final = circ_init + circ_weights\n",
    "    \n",
    "    # Add correct output to y_sim\n",
    "    \n",
    "    y_sim = np.hstack((y_sim, get(circ_final, 'statevector', nice = False).reshape(2, 1)))\n",
    "    \n",
    "y_sim = np.real(y_sim[:, 1:])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Run the states over the circuit\n",
    "#### 4. Measure the results, get the count distribution and infer back the amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulator = Aer.get_backend('qasm_simulator')\n",
    "\n",
    "# For Mathematical Representation\n",
    "results = execute(circ2, simulator).result()\n",
    "\n",
    "# Count Results\n",
    "counts = results.get_counts(circ2)\n",
    "\n",
    "# Plot Histogram\n",
    "plot_histogram(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this histogram, we should be able to infer the resulting statevector. In this case it's\n",
    "\n",
    "$$|\\psi \\rangle = \\alpha |0\\rangle + \\beta |1\\rangle$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\alpha^2  = P(0|\\psi)$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\beta^2 = P(1|\\psi)$$\n",
    "\n",
    "In general, this can be expressed as \n",
    "\n",
    "$$|\\psi \\rangle = \\sum_{i}^{N} \\sqrt{P (\\psi \\searrow s_i)} \\ |s_i\\rangle, \\text{where } s_i \\text{ are the basis states}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this formula, we can calculate the amplitude from probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Forward Propagation by Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_sim(W, X, hist = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform Forward Propagation by Simulation using a Quantum Circuit\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    W: ndarray\n",
    "        (2, 2) weights matrix used for the circuit\n",
    "    X: tuple\n",
    "        Contains 'gate' and 'CBS' used for initializing circuit\n",
    "    hist: bool (= True)\n",
    "        Determien whether to plot histogram\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    y_hat: \n",
    "        Predicted value from forward propagation\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Build Circuit\n",
    "    circ_init = init_special(*X)\n",
    "\n",
    "    circ_w = circ_from_weights(W)\n",
    "\n",
    "    circ_full = circ_init + circ_w\n",
    "\n",
    "    circ_full.draw('mpl')\n",
    "    \n",
    "    # Simulate Circuit\n",
    "    simulator = Aer.get_backend('qasm_simulator')\n",
    "\n",
    "    # For Mathematical Representation\n",
    "    results = execute(circ_full, simulator).result()\n",
    "\n",
    "    # Count Results\n",
    "    counts = results.get_counts(circ_full)\n",
    "\n",
    "    # Plot Histogram\n",
    "    if hist:\n",
    "        plot_histogram(counts)\n",
    "\n",
    "    # Build Amplitude Vector\n",
    "    if len(counts.keys()) == 2:\n",
    "        result_dist = np.array(list(counts.values()))\n",
    "\n",
    "        amplitudes = np.sqrt(result_dist / np.sum(result_dist)).reshape(2, 1)\n",
    "\n",
    "        aae(np.linalg.norm(amplitudes), 1)\n",
    "\n",
    "        y_hat = amplitudes\n",
    "\n",
    "    elif list(counts)[0] == '0':\n",
    "        y_hat = zero\n",
    "\n",
    "    else:\n",
    "        y_hat = one\n",
    "        \n",
    "    assert y_hat.shape == (2, 1)\n",
    "        \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Optimization Implementation Using Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to train the model, we are going to use the stochastic function we built above with a little modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is checked, the training can start. Implement the code by the following order\n",
    "\n",
    "    1. Create train/test data (X_train, y_train, X_test, y_test)\n",
    "    2. Initialize weights (W)\n",
    "    \n",
    "    Loop\n",
    "    \n",
    "    3. Forward Propagation (y_hat)\n",
    "    4. Define Cost Function (L)\n",
    "    5. Back Propagation (grad) * -2\n",
    "    6. Gradient Descent (W)\n",
    "    \n",
    "    Finally\n",
    "    \n",
    "    7. Normalized the matrix\n",
    "    \n",
    "The dataset is created before the model is trained\n",
    "The input of the 'train' method are\n",
    "    1. X-values\n",
    "    2. y-values\n",
    "    3. Weight initialization type ( = None ('random') )\n",
    "    4. Number of iterations (= 50)\n",
    "    5. Learning rate (= 0.1)\n",
    "    6. Loss / mse print frequency ( = 10)\n",
    "    7. Random seed ( = 0)\n",
    "    8. Print weights ( = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stoch1(X_sim, X_sim_set, y, init_weights = 'random', gd_method = 'batch', epoch = 50, \n",
    "          learning_rate = 0.1, print_frequency = 10, seed = 0, print_weights = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Train model from (X, y) pairs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_sim: tuples\n",
    "        Containg tuples for initializing circuit\n",
    "    X_sim_set: \n",
    "        Contaitn X-values for training\n",
    "    y: ndarray\n",
    "        y-values for the corresponding X-values\n",
    "    init_weights: str\n",
    "        'random' or 'zeros'\n",
    "    gd_method: str\n",
    "        'batch' or 'stochastic' (weights update after every trainign examples)\n",
    "    epoch: int\n",
    "        Number of weights updates through forward / backpropagation over entire dataset\n",
    "    learning_rate: float\n",
    "        Step size for weights update\n",
    "    print_frequency: int\n",
    "        Loss / mse print frequency\n",
    "    seed: int\n",
    "        Used for pseudo-randomly generate matrix\n",
    "    print_weights: bool\n",
    "        Show weights at the end for visual comparison\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    W: ndarray\n",
    "        Final weights after training\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Weights initialization\n",
    "    if type(init_weights) == str:\n",
    "        W = initialize(init_type = init_weights, seed = seed)\n",
    "    else:\n",
    "        W = init_weights\n",
    "        \n",
    "    view(W)\n",
    "\n",
    "    print('Training: \\n')\n",
    "        \n",
    "    for n in range(epoch):\n",
    "\n",
    "        # Iterate over training examples (y.shape)\n",
    "        for i in range(y.shape[1]):\n",
    "\n",
    "            # Reshape the individual training examples\n",
    "            X_stoch = X_sim_set[:, i].reshape(2, 1)\n",
    "            \n",
    "            print('X_stoch')\n",
    "            view(X_stoch)\n",
    "            \n",
    "            y_stoch = y[:, i].reshape(2, 1)\n",
    "\n",
    "            print('y_stoch')\n",
    "            view(y_stoch)\n",
    "            \n",
    "            # Foward Propagation\n",
    "            y_hat = forward_prop_sim(W, X_sim[i])\n",
    "            \n",
    "            print('y_hat')\n",
    "            view(y_hat)\n",
    "\n",
    "            # Compute cost\n",
    "            L = cost(y_stoch, y_hat)\n",
    "\n",
    "            # Compute mse\n",
    "            mse = cost(W, M_correct)\n",
    "\n",
    "            # Backpropagation\n",
    "            grad = back_prop(y_stoch, y_hat, X_stoch) * -2 # Multiply by a factor of -2 from above\n",
    "            \n",
    "            print('grad')\n",
    "            view(grad)\n",
    "\n",
    "            # Gradient Descent (The bigger the learning rate, the faster the model trains <= 1)\n",
    "            W = gradient_descent(W, grad, learning_rate = learning_rate) \n",
    "            \n",
    "            print('W')\n",
    "            view(W)\n",
    "\n",
    "        # Print loss and mse\n",
    "        if n % print_frequency == 0:\n",
    "            print(f'{n}th epoch, loss = {np.round(L, 6)}, mse = {np.round(mse, 6)}')\n",
    "        \n",
    "    # Normalized W matrix\n",
    "    \n",
    "    W /= np.linalg.norm(W, axis = 0)\n",
    "        \n",
    "    # Print weights\n",
    "    if print_weights:\n",
    "        print('\\n')\n",
    "        print('Final Weights')\n",
    "        view(W)\n",
    "        print(f'MSE: {mse}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare and contrast this version to that of classical stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: \n",
      "\n",
      "0th epoch, loss = 0.619035, mse = 0.396345\n",
      "\n",
      "\n",
      "Final Weights\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.702525470966782 & 0.793077110716206\\\\0.711658599781455 & 0.609121249389671\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0.702525470966782, 0.793077110716206],\n",
       "[0.711658599781455, 0.609121249389671]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.3963450165116302\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.70252547, 0.79307711],\n",
       "       [0.7116586 , 0.60912125]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stoch0(X_sim_set, y_train, init_weights = 'random', gd_method = 'stochastic', epoch = 1, learning_rate = 0.01\n",
    "                      ,print_frequency=5, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5488135 ],\n",
       "       [0.60276338]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "np.dot(np.random.rand(2, 2), zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.548813503927325 & 0.715189366372419\\\\0.602763376071644 & 0.544883182996897\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0.548813503927325, 0.715189366372419],\n",
       "[0.602763376071644, 0.544883182996897]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: \n",
      "\n",
      "X_stoch\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1.0\\\\0.0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1.0],\n",
       "[0.0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_stoch\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.707106781186548\\\\0.707106781186547\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0.707106781186548],\n",
       "[0.707106781186547]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.676762744615866\\\\0.736201186836859\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0.676762744615866],\n",
       "[0.736201186836859]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}-0.060688073141363 & 0.0\\\\0.0581888113006235 & 0.0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[-0.060688073141363, 0.0],\n",
       "[0.0581888113006235, 0.0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.554882311241461 & 0.715189366372419\\\\0.596944494941582 & 0.544883182996897\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0.554882311241461, 0.715189366372419],\n",
       "[0.596944494941582, 0.544883182996897]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_stoch\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.707106781186547\\\\0.707106781186547\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0.707106781186547],\n",
       "[0.707106781186547]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_stoch\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1.0\\\\0.0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1.0],\n",
       "[0.0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.999511599482467\\\\0.03125\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0.999511599482467],\n",
       "[          0.03125]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}-0.000690702635764838 & -0.000690702635764838\\\\0.0441941738241592 & 0.0441941738241592\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[-0.000690702635764838, -0.000690702635764838],\n",
       "[   0.0441941738241592,    0.0441941738241592]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.554951381505038 & 0.715258436635996\\\\0.592525077559166 & 0.540463765614481\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0.554951381505038, 0.715258436635996],\n",
       "[0.592525077559166, 0.540463765614481]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_stoch\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.707106781186547\\\\-0.707106781186547\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[ 0.707106781186547],\n",
       "[-0.707106781186547]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_stoch\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.0\\\\1.0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0.0],\n",
       "[1.0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0\\\\1\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0],\n",
       "[1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.0 & 0.0\\\\0.0 & 0.0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0.0, 0.0],\n",
       "[0.0, 0.0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.554951381505038 & 0.715258436635996\\\\0.592525077559166 & 0.540463765614481\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0.554951381505038, 0.715258436635996],\n",
       "[0.592525077559166, 0.540463765614481]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th epoch, loss = 0.0, mse = 0.398195\n",
      "\n",
      "\n",
      "Final Weights\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.683586452538673 & 0.797842493243607\\\\0.729869551293649 & 0.602865951912052\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0.683586452538673, 0.797842493243607],\n",
       "[0.729869551293649, 0.602865951912052]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.39819473779759434\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.68358645, 0.79784249],\n",
       "       [0.72986955, 0.60286595]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stoch1(X_sim, X_sim_set, y_sim, init_weights='random', epoch = 1, learning_rate = 0.1, print_frequency = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
